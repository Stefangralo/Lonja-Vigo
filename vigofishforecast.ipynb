{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method NDFrame.head of date        name               \n2017-02-01  ABADEJO                 42.4\n            ACEDIA-GOLLETAS          0.0\n            ACEDIAS                  1.6\n            AGUJAS                   0.0\n            ALACHA                 310.0\n                                   ...  \n2019-09-09  VARIADO                 52.0\n            VIEIRAS                  0.0\n            VIEJAS                   0.0\n            VOLADOR- POTA-POTON    125.0\n            XOUBA                    0.0\nLength: 172430, dtype: float64>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.tsa.stattools as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf ,plot_pacf\n",
    "from pmdarima.arima import auto_arima\n",
    "from pmdarima.preprocessing import FourierFeaturizer\n",
    "import statsmodels.api as sma\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "#Definimos la funcion para importar el JSON con los datos de las ventas\n",
    "def import_data():\n",
    "    with open(\"fishVigo.json\") as file:\n",
    "        dataset = pd.read_json(file)\n",
    "        return dataset\n",
    "\n",
    "#Asignamos el resultado de la funcion a una variable y lo convertimos a DataFrame\n",
    "df = pd.DataFrame(import_data())\n",
    "df.date = pd.to_datetime(df.date)\n",
    "\n",
    "#Seteamos un indice basado en la fecha y la especie y eliminamos las filas duplicadas quedandonos con la primera de ellas\n",
    "df = df.set_index(['date','name'])\n",
    "df = df[~df.index.duplicated(keep = 'first')]\n",
    "\n",
    "#Convertimos la columna de cantidades a numerico\n",
    "df['quantity'] = df['quantity'].str.replace('Kg.','')\n",
    "df['quantity'] = df['quantity'].str.replace('.','')\n",
    "df['quantity'] = df['quantity'].str.replace(',','.')\n",
    "df['quantity'] = pd.to_numeric(df['quantity'])\n",
    "df = df['quantity']\n",
    "\n",
    "#Mediante reshape creamos registros a 0. Ahora para todos los días tenemos datos de todas las especies\n",
    "df = df.unstack().fillna(0).stack()\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StationarityTests:\n",
    "    def __init__(self, significance=.05):\n",
    "        self.SignificanceLevel = significance\n",
    "        self.pValue = None\n",
    "        self.isStationary = None\n",
    "        \n",
    "    def ADF_Stationarity_Test(self, timeseries, printResults = True):\n",
    "        #Dickey-Fuller test:\n",
    "        adfTest = sm.adfuller(timeseries, autolag='AIC')\n",
    "        \n",
    "        self.pValue = adfTest[1]\n",
    "        \n",
    "        if (self.pValue<self.SignificanceLevel):\n",
    "            self.isStationary = True\n",
    "        else:\n",
    "            self.isStationary = False\n",
    "        \n",
    "        if printResults:\n",
    "            dfResults = pd.Series(adfTest[0:4], index=['ADF Test Statistic','P-Value','# Lags Used','# Observations Used'])\n",
    "            #Add Critical Values\n",
    "            for key,value in adfTest[4].items():\n",
    "                dfResults['Critical Value (%s)'%key] = value\n",
    "            print('Augmented Dickey-Fuller Test Results:')\n",
    "            print(dfResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sacamos el porcentaje de cuanto ha aportado al total de capturas cada especie\n",
    "total = df.sum()\n",
    "groupName = df.groupby('name').sum()\n",
    "percentage = groupName/total*100\n",
    "top = 5\n",
    "\n",
    "#Seleccionamos aquellas especies cuyo aporte a las capturas supone mas del 5%\n",
    "percentage = pd.DataFrame(percentage[percentage>top])\n",
    "selectedSpecies = list(percentage.index.unique())\n",
    "percentage.columns = ['quantity']\n",
    "percentage = percentage.reset_index()\n",
    "\n",
    "#Dibujamos\n",
    "bar = go.Figure(go.Bar(x = percentage.name, y = percentage.quantity))\n",
    "bar = bar.update_layout(title='Top '+str(top)+' Cuota',yaxis_title=\"% Porcentaje\")\n",
    "bar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraemos del df aquellos datos pertenecientes a las especies seleccionadas\n",
    "for x in selectedSpecies:\n",
    "    dfTop = df.xs(x, level=1, drop_level=False)\n",
    "    \n",
    "    dfTop = pd.DataFrame(dfTop)\n",
    "    \n",
    "    dfTop.columns = ['quantity']\n",
    "    dfTop = dfTop.reset_index()\n",
    "    \n",
    "    fig = go.Figure(data = go.Scatter(x=dfTop.date, y=dfTop.quantity),\n",
    "                                        layout_title_text = x)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in selectedSpecies:\n",
    "    df1 = df.xs(x, level=1, drop_level=False)\n",
    "\n",
    "    sTest = StationarityTests()\n",
    "    sTest.ADF_Stationarity_Test(df1, printResults = True)\n",
    "    print(\"Is the time series \" + x + \" stationary? {0}\".format(sTest.isStationary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPredict = df.xs('GALLO', level=1, drop_level=True)\n",
    "\n",
    "dfPredict = dfPredict.asfreq('B',)\n",
    "\n",
    "dfPredict[dfPredict.isnull()==True] = 0\n",
    "\n",
    "model = sma.tsa.statespace.SARIMAX(dfPredict, order=(0, 1, 1), seasonal_order=(0, 1, 1, 12))\n",
    "results = model.fit()\n",
    "#plot = results.plot(500, 600)\n",
    "results.plot_diagnostics(figsize=(18, 8))\n",
    "\n",
    "#model.geterrors()\n",
    "\n",
    "pred = results.get_prediction(start=pd.to_datetime('2018-01-03'), dynamic=False)\n",
    "pred_ci = pred.conf_int()\n",
    "pred_ci.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in selectedSpecies:\n",
    "    y = \"./plots/test\" + x + \".png\"\n",
    "    \n",
    "    dfDecomp = df.xs(x, level=1, drop_level=True)\n",
    "    dfDecomp = dfDecomp.asfreq('B',)\n",
    "    \n",
    "    dfDecomp[dfDecomp.isnull()==True] = 0\n",
    "    \n",
    "    result = seasonal_decompose(dfDecomp, model='additive')\n",
    "    fig = result.plot()\n",
    "    fig.savefig(y,dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in selectedSpecies:\n",
    "    filenameACF = \"./plots/ACF-\" + x + \".png\"\n",
    "    filenamePACF = \"./plots/PACF-\" + x + \".png\"\n",
    "    \n",
    "    dfAutoCorrelation = df.xs(x, level=1, drop_level=True)\n",
    "    dfAutoCorrelation = dfAutoCorrelation.asfreq('B',)\n",
    "    \n",
    "    dfAutoCorrelation[dfAutoCorrelation.isnull()==True] = 0\n",
    "    \n",
    "    figACF = plot_acf(dfAutoCorrelation,lags=150, zero=False)\n",
    "    figPACF = plot_pacf(dfAutoCorrelation, lags=150, zero=False)\n",
    "    figACF.savefig(filenameACF,dpi=500)\n",
    "    figPACF.savefig(filenamePACF,dpi=500)\n",
    "#Se puede observar en los graficos ACF y PACF que la seasonality es 5 (semanal en nuestro dataindex sin fines de semana), además podemos ver otras seasonalitys mas grandes, sin embargo varian dependiendo de la especie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = FourierFeaturizer(65)\n",
    "#exog = trans.fit_transform(dfARIMA)\n",
    "#exog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfARIMA = df.xs('GALLO', level=1, drop_level=True)\n",
    "dfARIMA = dfARIMA.asfreq('B',)\n",
    "\n",
    "dfARIMA[dfARIMA.isnull()==True] = 0\n",
    "\n",
    "stepwise_model = auto_arima(dfARIMA, start_p=1, start_q=1,\n",
    "                           max_p=8, max_q=8, m=5,\n",
    "                           seasonal=True,\n",
    "                           d=None, D=None, trace=True,\n",
    "                           error_action='ignore',  \n",
    "                           suppress_warnings=True, \n",
    "                           stepwise=True)\n",
    "print(stepwise_model.aic())\n",
    "result = stepwise_model.fit(dfARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dfARIMA.loc[:'2019-01-06'], dfARIMA.loc['2019-01-07':]\n",
    "result = stepwise_model.fit(train)\n",
    "predction = result.predict_in_sample(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "date\n2019-01-07    46164.52\n2019-01-08    31111.52\n2019-01-09    17091.90\n2019-01-10    14033.50\n2019-01-11    10529.30\nFreq: B, dtype: float64"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  }
 ]
}